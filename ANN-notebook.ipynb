{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Currently, one of the most common (and accurate) methods for conducting a Blood\n",
    "Smear is manually. The goal of this project is to develop a neural network that can classify WBCs from images as part of an eventual effort to automate the procedure without a significant loss in accuracy. By automating this process, we can not only speed it up, but we also reduce the amount of human labor required to conduct a test, thus lowering the overall cost.\n",
    "\n",
    "The dataset for this project is a collection of ~12,500 images that are 240 x 320. The images contain several RBCs and a single, highlighted WBC. Each WBC falls into one of four categories: Eosinophil, Lymphocyte, Monocyte, or Neutrophil. The dataset can be found on Kaggle [here](https://www.kaggle.com/paultimothymooney/blood-cells). Through accurate classification, accurate proportions of each WBC type could be calculated and checked for normalcy. Additionally, cell images could be further inspected for abnormalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "    from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Let's begin by taking a look at an example of each of the four types of WBC we'll be attempting to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbc_types = ['EOSINOPHIL', 'NEUTROPHIL', 'LYMPHOCYTE', 'MONOCYTE']\n",
    "wbc_df = pd.DataFrame(columns=['file_name', 'type'])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for i, wbc_type in enumerate(wbc_types):\n",
    "    path = 'dataset2-master/images/TRAIN/' + wbc_type + '/'\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['file_name', 'type'])\n",
    "    df['file_name'] = files\n",
    "    df.fillna(value=wbc_type, inplace=True)\n",
    "    wbc_df = wbc_df.append(df)\n",
    "    \n",
    "    image = load_img(path + files[0])\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(wbc_type)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='type', data=wbc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images show that each of the four cell types are quite easily visually differentiable. Additionally, we can see from the countplot that our training data is very well balanced with ~2,500 images of each WBC cell type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction\n",
    "\n",
    "Since the input to be classified are image files, we will be using a Convolutional Neural Network for these purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(60,80,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset2-master/images/TRAIN',  # this is the target directory\n",
    "        target_size=(60, 80),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'dataset2-master/images/TEST',\n",
    "        target_size=(60, 80),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=9957 // 32,\n",
    "        epochs=30,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=2487 // 32)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
